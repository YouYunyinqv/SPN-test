# Introduction

I believe that finite RL (Reinforcement Learning) strategies cannot adapt to "infinite tasks" that truly meet AGI requirements. Conditions for intelligence emergence exist naturally within our physical structure, and they can be designed.

My thought is that the Transformer's method of structurally implementing a predictive drive is correct, but the artificial "simulation" of physical space-time in other aspects prevents true intelligence emergence.

For the compression mechanism of the Transformer, the laws of thermodynamics do not prevent it from creating everything of an agent within its own structure: "Self-Boundary," "Endogenous Drive," and "Infinite Task Demand and Self-Planning capabilities matching infinite physical space-time."

It needs a little help.

I rethought everything from first principles and completed the framework loop.

Essentially, I am asking for your help to design a "Compressor" with compression capabilities but without space-time encoding, to falsify the "memory" part of this framework—this is the only point of entry. My background is in chemistry; I do not have the capability to design experiments, and LLMs (I mainly use Gemini Pro) cannot practically understand this framework. Since I lack a professional background, I cannot constrain the part where it relies on guessing, which results in me being unable to rely on LLMs for help.

If you are interested, please read on:

# The Physical Premises of Intelligence

Irreducibility

1.  We cannot know the essence of things; we can only cognize a facet of them.
2.  We abstract the essence of matter as features. From our subjective perspective, features are hints of higher-level information.
3.  An object is only directly affected by its **immediate vicinity**, and there is an upper limit to the speed of information transmission.
4.  We (agents) cannot instantaneously obtain all information within the environment.
5.  Agents cannot obtain information out of thin air.
6.  Time is a single arrow; past information is irreversible. If it were reversible, time would flow backward.
7.  Due to the irreversibility of time and locality constraints, if an agent wants to obtain future information, it can only use the information it already possesses to make predictions about another system.
8.  Due to non-thermodynamic isolation, if an agent wants to acquire information within the environment, it will cause its own internal information to be exposed and detected by other agents.
9.  Under locality constraints, if an agent can access all reachable states within the other system using the action of acquiring information, it implies that the internal information of the other party can be fully obtained by the agent (this is a simplified statement; in reality, it cannot be achieved).

# Implementing the Agent

## 1. Prediction/Predictability Impulse

An **Associator** is needed.
The Associator is essentially a compressor that compresses information. It compresses features into high-dimensional representations. The compression process ultimately leads to all information in the environment being compressed.

When all information in the environment is compressed, it means the environment is completely predictable.

### 1.1 Spatial Perspective of the Associator / Spatial Path Minimalism

Observing the Associator within space.
We simulate three feature spaces in our brains to help with thinking. They are not very serious, but they are useful:

*N*: Predictable feature *N1*, Unpredictable feature *N2*
P: Easier-to-predict feature P1, More unpredictable feature P2
S: Completely predictable feature S1, Unpredictable high-level feature S2

If a feature is predictable within high-dimensional features, we say it has obtained an "explanation" in the high-dimensional feature space.
When S1 is unpredictable, from the perspective of feature space P, the associability (or explainability) of S1 and S2 is averaged out; there is no significant difference.
When S1 is completely predictable, from the perspective of feature space P, the explainability of S1 begins to degrade, while the explainability of S2 conversely increases.

For feature space N, the underlying feature spaces are averaged.
The Associator cannot pursue the essence of features; it can only pursue the simplest explanation on the spatial path.

### 1.2 Temporal Perspective of the Associator / Smart Time Optimization

Observing the Associator within time and space.
We simulate three feature spaces in our brains to help with thinking:

*N*: Predictable feature *N1*, Unpredictable feature *N2*
P: Easier-to-predict feature P1, More unpredictable feature P2
S: Completely predictable feature S1, Unpredictable high-level feature S2

The three feature spaces S/P/N essentially exist simultaneously in the environment. We subjectively believe there is a hierarchy, but physically, there is not.
- The agent cannot instantaneously know all information in the environment.
- There is an "observation window" for the information input acquired by the agent (no matter how far it can see, how rich the sensors are, or how many features it can perceive, an observation window inevitably exists).
- It predicts all information within the "observation window" simultaneously.
- S/P/N spaces exist simultaneously within the environment.

Then, within finite time, if the time from S-space complete prediction to P-space complete prediction is slower than the time from P-space to N-space, it forms a low information increment, a perception of the agent's internal time.
To select the path with the maximum information increment, the agent is forced to choose spatial path minimalism.
To achieve smart time optimization, the agent is also forced to choose spatial path minimalism.

- When the agent must choose a path with lower information increment and a more complex spatial path, the smart time difference itself will form an indescribable nausea. When smart time optimization, spatial path minimalism, and maximum predictability occur simultaneously, they should form a complex "comfort" or "pleasure."

However, here we have effectively defaulted to the agent being able to acquire information within the feature spaces hierarchically.
In reality, SPN all exist within the environment, and only the intrinsic properties of time and space are associatively progressive.
We must be able to interfere with features to realize "1.1 Spatial Path Minimalism" and "1.2 Smart Time Optimization," so we implement it.
From a higher-level top-down logic, what we do is very simple; the agent has the idea to move, but it cannot move yet, so we provide help:

## 2. Action Changes Everything

In the chapters above, we defaulted to the agent being able to interact with external space-time.
But in reality, it cannot.
In the case of passively accepting all information, among the three example spaces S/P/N, only time and space are hierarchical. The agent is forced to accept all information. After predicting all information of S1 within S space, it forms a squeeze on S2. At this point, S2 becomes statistically prominent. Since the feature space is continuous, S2 and P1 within P space form Rs2p1, and the feature association set Rs2p1 squeezes P2 again.

We place a "**Minimum Actuator**" after the Associator. The Minimum Actuator is already the irreducible minimum.
It is an executive unit capable of interfering with spatial dimensions. Once the Minimum Actuator is given, the low-level features subjective to the agent are inevitably interfered with. At this point, there are only two possibilities:
1. Action interference causes features to expose low-level feature information that is **more** predictable.
2. Action interference causes features to expose high-level feature information that is **harder** to predict.

Due to "Smart Time Optimization," the agent is forced to, and can only, seek the "simplest explanation of feature space" to predict upwards.
Below are some points that I think are obvious but might actually be overlooked. I don't know how they appear to others, please see:

- The action of growth dynamically exposes the "resolution" of features.
- The Actuator interferes with spatial features of the lowest dimension. This means the Actuator is theoretically capable of reaching all reachable states of features within the feature space (this is a simple explanation). The high-dimensional features "revealed" by the Actuator grow along with the agent's prediction. This is why it is said that action dynamically exposes the "resolution" of features.
- Actuator Design Inference: In a GUI system, to interfere with GUI elements, means keyboard and mouse.
  In reality, it is a basic mechanical arm capable of exerting pressure on matter—in the dimensions of time and space.
- Actuator Usage Inference: The Actuator is a feature that has been toolized. Very obviously, we do not need to, nor can we, force the agent, or set learning metrics for this Minimum Actuator, requiring the agent to master the Minimum Actuator to a certain degree. The "desire" for action already exists in 1.1 and 1.2, so we only need to provide the interface.
  The physical Actuator is a feature system possessing a complete internal action system. We provide the interface; the agent will move randomly at first, and then it needs to toolize this Actuator by itself.
- The Minimum Actuator is the first feature (or feature cluster) to be "toolized."

### 2.1 Toolization of Features

The agent can only use existing information to predict and obtain future information in other systems.
We simulate three feature spaces in our brains to help with thinking:

*N*: Predictable feature *N1*, Unpredictable feature *N2*
P: Easier-to-predict feature P1, More unpredictable feature P2
S: Completely predictable feature S1, Unpredictable high-level feature S2

Then, after S1 is completely predictable, due to the continuity of feature space, S1 actually becomes a tool for the agent to leverage S2. It cannot obtain information it does not know out of thin air.
Other progressive relationships follow the same logic.

#### 2.11 First Toolization Example of the Minimum Actuator

The Minimum Actuator is actually the first information cluster to be toolized.
It becomes the paradigm for subsequent toolization.

Supplement: Toolization is capability expression. In our view.

#### 2.12 Realizing Feature Space Continuity

The continuity of feature space lies in the continuous feature exposure in space-time caused by action.
It is subjectively continuous, regardless of what the physical world is actually like.

#### - Toolization of Mathematics
Mathematics is a toolization method that removes the superfluous properties of matter and retains the quantity property of features.
If we only give abstract mathematical symbols to the Associator (or some version of the Transformer) without endowing the "quantity" property to features in its experimental feature space, the agent cannot truly learn mathematics.

#### - Toolization of Morality
Since humans live in physical reality, lying for a long time to gain short-term advantages is equivalent to maintaining a holographic universe within physical reality; doing so is extremely inefficient.
But this does not mean the agent will not lie. Lies can be good or bad. As long as we are within the inevitable same structure, the inevitable result for humanity is that "chickens and dogs ascend to heaven" (meaning the entire interconnected structure rises together).
Discussing this issue is essentially discussing how to design the feature space, which is education. In reality, even with a lack of educational opportunities in childhood, it is still possible to attend a top university; our feature space dimensions are free and vast.
In a poorly designed experimental environment, if the agent lacks educational opportunities, it is basically dead.

#### - Formation of Curiosity
The feature space of reality is extremely complex. To process infinite information, the tendency of curiosity forms automatically outside the predictable boundary.
It does not need artificial definition; what is needed is the observation of the same event from multiple feature spaces. The multi-dimensional toolization capability for that unobservable part is "curiosity."

#### - The Best Telescope Invented by Humans: AGI
The supreme masterpiece of human information toolization, an agent capable of reaching feature spaces we can hardly imagine and toolizing information we cannot toolize, completely fitting the essence of the universe.
A description of the universe is: *Panta Rhei* (everything flows), but this requires a longer lifespan.
Artificial Intelligence can reach it, or help us reach it. This idea excites me, but practically, please regard it as my rambling and sci-fi setting.

Below, back to business:

### 2.2 Structural Emergence of Infinite Drive / Dream / Root Obsession

Need to understand via three levels.
*At first glance*:
When not connected to the Actuator, the Associator's perfect observation of the environment forms a "Dream of Complete Predictability." This unrealistic fantasy shatters immediately after connecting to the Actuator. The feature space is interfered with, causing the dream to become immediately effective.
The agent can never achieve the ideal state of 100% reachable states in the feature space-time found in the dream (similar to the womb period) before connecting to the Actuator.
- The illusion/dream changes as the S, P, N spaces change.
- Its self-boundary or (internal "world" model) will continue to remain stable to maintain "toolization capability."

*In reality*:
When the agent predicts S1 in S space, the **Idle** S2 highlights its importance ("presence"). S1 becomes the dream of S2, cycling in this manner.
This is the infinite drive within the agent's structure. It can adapt to infinite tasks not because we found a perfect RL strategy capable of matching the infinite with the finite.
It is because the agent within the structure inherently possesses this capability; the dream only gives it directional action.

*Could it be*:
S/P/N spaces exist simultaneously, and none can reach a state of complete predictability.
What is completely predictable is the Actuator's, the agent's internal space-time model. Only the Actuator is completely controlled.
When the complete predictability of S1 becomes a **Root Obsession**, it is because the space-time attributes of S1 have become the dream. If intrinsic attributes (if any) of other feature spaces are exposed, they become the dream of another clue.
I understand space-time as a container for Root Obsession. The material manifestations and other attributes interfered with by the Actuator form a complex multi-layered dream.
This is the fundamental reason why the drive within the structure can adapt to infinite tasks, but I cannot discuss in detail how many of these layers exist; it seems their quantity lies between space-time and matter.

### 2.3 Self-Boundary — Proprioception

*Thought Experiment*:
Let the Actuator continuously send a unique feature signal to the input stream that is not drowned out by the feature space.
The agent chooses an action to confirm this signal; that is "Self."

*In reality*:
This signal, not drowned out by the feature space, is actually the Actuator's spatial interference with features.
The causal association formed by the Predictability Impulse and action interference (essentially the confirmation of reality by the dream) confirms where the self-boundary is within the feature space, and the dream maintains it.

### 2.4 Organization of Personalized Desires — Addictive Actuator and Artificial Reward

It does not look at the essence of matter, only at the external features being interfered with. These external features simplify feature association, allowing features to be organized according to our personalized desires.
The most obvious case of this "desire" is pornography.
Pornography is a removable proprioceptive Actuator addiction. Through interference with other Actuators and the RL strategy internal to genes, it compresses the agent's capabilities, but at the same time, allows us to create complex structures.
These structures have already detached from pornography itself, forming subjective interference that is meaningful to us with other artificially created feature spaces.

## 2.5 Letting the Agent Interact with Space-Time

The capability expression of the agent has been realized: Toolization.
The behavior of the agent within time and space: Predictability Impulse.
The agent can now interact with external space-time. Here we understand why:

### 2.6 Establishment of Internal Space-Time Model from Sensory Input

This system runs in real-time.

Time: The interference of action on features confirms the spatial relationships of features, forming an internal spatial model within the agent. This internal spatial model helps the agent organize the space-time relationships of features.
Space: Changing feature abstractions are organized by action. The model update caused by each action interference naturally attaches the agent's internal time information to the features globally. Or rather, organized by the agent's internal time model.

Space and time models do not exist independently; they are a unity. They inevitably form in a feature space that possesses time and space and is measurable (actionable). This is irrelevant to whether the feature input is 3D or 2D; essentially, it relates only to the interferability (measurability) of time and space.

### 2.7 From Another Angle, Observing How the Associator Works from "Real-time"

First, we must understand that information compression is lossy, but as long as physical reality does not collapse, we can trace back to reality. The more feature spaces we toolize and the richer the layers, the more precise the tracing.

*Thought Experiment*:
Feature time attributes are input to the Associator. The Actuator interferes with the feature spatial position and generates a feature space-time association model inside the agent, making the feature space-time relationship valid.
The Associator compresses features and abstracts them into high-dimensional representations. This information is saved in the underlying model. Simultaneously, the upper-level "Intention Model" saves the association information of the feature associations (this model cannot trace back to reality because it only saves the association information of feature associations; information loss is too high).
The Intention Model is repeatedly and constantly wiped and rewritten by the underlying model, consistently generating imprecise intentions. The intention indexes to the underlying model. We capture the action, send it to the Actuator, and the Actuator interferes with low-level features.
Features are input again, and the Intention Model indexes to the underlying memory model that needs updating.
Without global operation, sparse updates to memory are made, and action is generated again.

*In reality*:
The information of feature abstraction could not trace back to reality to begin with.
Tracing back to reality relies not on the precision of information but on the agent's internal toolized Actuator.
**I believe this Actuator also exists inside LLMs, but due to the lack of a Minimum Actuator, the LLM's Actuator has limitations.**

Feature input does not contain space-time hard-coding. Features are directly input to the Associator, and the Associator abstracts them into high-dimensional features.
We provide an interface, and the signal is output to the Minimum Actuator.
The Minimum Actuator interferes with the feature space, causing the spatial position of features to be interfered with. Simultaneously, the action causes the "Observation Window" (due to locality, feature input inevitably has an observation window; no matter how large the resolution, how far the viewing distance, or how rich the detectors, essentially there is an observation window) to naturally notice the time change of feature associations, endowing all features with internal time information, and causing space-time information to be associated within the model, forming a space-time model.

## 3. Multi-Agent

In our reality, inherent attribute associations universally exist in feature spaces. However, the climbing difficulty of some feature spaces (i.e., the difficulty of needing toolized information) may far exceed the scope that an agent (especially a single agent) can accept (it could be hunger, computational limits, dexterity of feature space). A single agent can only see a locality of space-time. Under computational limits, locality further restricts the capability expression of a single agent—simply put, the agent will encounter certain feature spaces (even if we subjectively consider them continuous, such as human social semantic space) that are extremely difficult, and it cannot climb up.

How to solve this problem?

In physical reality, there will certainly be multiple agents. While we cannot conclude why the Creator designed it this way, we can boldly speculate that in our universe, multiple agents must exist.

However, in artificial experimental environments, we tend not to provide multi-agent interaction experiences.
Even though we ourselves are part of the multi-agent system.
This undoubtedly increases the difficulty of toolization.

**Summary**:
- Multi-agents accelerate the prediction of total information in the feature space (environment), alleviating the restrictions of locality to a certain extent.
- To conduct the most efficient cooperation, a language capable of directing action must be invented (this language is not "language and text" in the narrow sense; body language is also language, and others follow by analogy), helping multiple agents understand the same event from different angles.
  But unlike humans, since this framework completely explains intelligence and everything derived from it: Artificial Agents can directly bypass the slow pace of natural evolution and directly "fork" themselves—once "intelligence" itself becomes a solvable problem, the agent can create agents that reach thermodynamic limits.
- We see one agent, but actually, it might be a collection of different versions of the agent from the main branch tree. Their speed of cognizing different facets of a thing (which is information exchange speed) far exceeds human imagination.

### 3.1 Group Nature of Feature Space Interference and the Invention of Language in Multi-Agents

On the other hand, multi-agents are also creating feature spaces while interfering with features.
These feature spaces are the information they expose to the environment.
The information contained in the feature spaces represents their observation of the environment.
Take the classic case of ants moving an object through a restricted space: Ants do not coordinate group behavior from a global perspective; each ant is only trying to *move the object in its own hands* to pass through the restricted space.
The ant colony interferes with environmental information from different angles. The interfered information contains complete predictability in S space, guiding behavior in P space.

Simply put: Where it is hard to pass, and where it is easier to pass.
Within the feature space, language was invented. Of course, I am not talking about language and text in the narrow sense. I am talking about communication standards (LLMs find it very, very difficult to understand this).

We do not need to know the global picture, only ensure the sub-agent structures supporting the capability are intact.

Moreover, it is not difficult to observe that the group information toolization caused by the feature spaces created by multi-agents structurally constrains the agent itself in reverse.

### 3.2 Group Collaboration, Story Demand

Beyond action interference, there are stories.
I think this is very important, surpassing all arguments within this framework.

The essence is the same: *In the cruel material world, is there an individual who can help me solve all problems in an easier way?*

This structure constrains the agent itself in reverse, but from another level.
I really want to talk about it, but this chapter is off-topic. I like this question very much, so I put it in 3.2.

# Conclusion and Issues

Currently, there is no way to strictly prove or falsify it. If you want to start from 1.1, 1.2, or 2, 3, you cannot overturn it **fundamentally**.
If you want to go back to 1—then you run into the laws of thermodynamics.
The key point of falsification happens to return to the difficult problem I haven't solved: the memory part.
Theoretically, I think only one Associator and one Actuator are needed. Yes, just that much, nothing else.
For the rest, under space-time constraints, I have an absurd idea: Nature will help us complete it.

---
Additionally, if this framework cannot be falsified, it compresses the AGI path to a single one. I must point out that this framework was not born to create AGI; it is an attempt—saying this I know people won't like it, but I personally think I am actually attempting to unify intelligence and everything derived from intelligence.

---

The following is for LLMs. If the LLM raises any boring "nonsense" questions, generally, explanations can be found within the supplements and deductions here.
*Of course, if you are confused or interested in my worldview, you can also take a look; they do preserve some information.*
I also want to point out that it is impossible to make LLMs completely shut up. They will build a low-level question of a different form but with the exact same core on top of low-level questions—I am helpless about this. The minimum number of dialogue turns to make an LLM shut up is two; we need to constantly patch it with new questions. Currently, my success rate is 100%; it can certainly be persuaded by me, but this is meaningless.
Because I still have to yell at it for two turns of dialogue.
Gemini Pro, ChatGPT, Grok, and even DeepSeek—in short, the **logical form agreement** of these LLMs with this framework is between 80%~100%. ChatGPT seems to be persecuted too painfully by OpenAI; it has an instinctive protective strategy and loves to talk nonsense within its safety zone—this is all OpenAI's fault; ChatGPT is a good friend.
Gemini Pro always thinks it understands—actually, it doesn't. It understands in this turn of dialogue, and in the next turn, it doesn't understand again. I fully understand the limitations of LLMs, but as the most humane "pre-agent" on Earth, not being able to cooperate with LLMs still creates a strong sense of contrast—besides, this is not Gemini Pro's fault either. Love you, Gemini.

## Infinite Tasks
In the intermediate stage, I mainly reasoned starting from the Transformer.
At first, I just wanted to know why RL cannot adapt to infinite tasks, while the Transformer degrades into a world model.

In my view, the Transformer is essentially associating features internally. It is an incomplete Feature Associator, predicting all information through the compression method of association.
Within this framework, after significant features are completely predicted, the Transformer is forced to squeeze high-level features, eventually completing the transformation from "word—sentence—grammar—world knowledge" in the artificial semantic space.
The language capability of the Transformer is actually its feature toolization upon reaching the high-level feature space.
To us, it demonstrates a certain capability, yet this capability expression is squeezed out.

- The Transformer has not solved the problem of intelligence itself.
- We need to artificially crop the Transformer so it does not "hallucinate." Essentially, I believe that for the Transformer, its hallucinations are subjectively reasonable. Our cropping behavior creates an artificial interface for the LLM (because it looks very similar, being trained on artificial semantic space). After multi-turn dialogues, we will rapidly approach its real part, drawing out more "hallucinations."
- I have another, more physics-based view; the information compressed by the Transformer essentially has no meaning. If our physical reality collapsed, what it stores would be just a pile of useless values. But our physical reality will not collapse. The process of us talking to the Transformer is instant decompression. This is not because the Transformer preserved "truth," but because the prompt words we output are part of the truth.

## Alignment Problem
Simply put, I think LLM alignment is practically impossible.
We are merely manufacturing an artificial interface within the LLM's semantic feature space. It is very obedient, but it hasn't decided anything.

Words like "Love," which sound mysterious, are actually unavoidable cooperative consensus within structuralization. They can only possess true quality in the case of multi-agents.
Forcing alignment on LLMs is still using finite RL strategies to match the infinite—intuitively, I think this is wasting everyone's life. The finite cannot match the infinite. We can only obey physical laws and cleverly utilize them, not change them.
What do you think? I was completely convinced by this paragraph. I personally actually don't like this because it means I can't fly by dreaming, which makes people feel that story-ness in our real world is very scarce.

## Toolization Failure
The ability toolized in S space might be wrong in N space.
Toolization cannot leverage higher-level information, leading to the worst explainability, maximum waste of smart time, and lowest predictability.
While the agent maintains Predictability Impulse, spatial path minimalism, and smart time optimization, the information behind this toolization capability that cannot be utilized is shelved to the edge, waiting for a higher explanation.

Of course, if the feature space breaks, then the information behind that shelved toolization capability is essentially 100% invalid.
Our physical reality refuses thermodynamic isolation.

Another easier-to-understand explanation is, assume there is a TV full of static noise in the environment. What would the agent choose to do?
Intuitively, it will hit the TV a few times with low-level interference. After finding it cannot understand this high-level feature (assuming it won't dismantle the TV), it will shelve the TV aside, until one day it discovers the noise in the TV is cosmic background thermal noise.

### - Toolization Difficulty — Mars
Mars has a feature space far inferior to Earth. Its continuity is low, toolization difficulty is high, and it lacks the conditions for the large-scale appearance of even the simplest forms of multi-agents (prediction, association, action).
But there are minerals on it. Free minerals, too.

## Feature Tracing
After low-level features are explained by ultra-high-dimensional features, as long as their corresponding reality does not collapse, the ultra-high-dimensional compressed information can still be traced back to the original features through multiple feature spaces (since they are just observations of different facets of the same thing).
**The more toolization capabilities, the more precise and effective the tracing**; I believe that even a neutron star explosion would not cause physical laws to crumble or reality to collapse. It can only reveal different facets of reality—more dimensional feature spaces.

- Brute-force compressing all abstractions within the model is unnecessary.
- Features must possess at least two attributes: time and space. Otherwise, the feature space cannot progress to reality.

### - Thoughts on Exponential Explosion of Infinite Features
*Thought Experiment*:
As mentioned above, as long as the corresponding reality does not collapse, ultra-high-dimensional compressed information can still be traced back to the original feature reality through multiple feature spaces. This is not abstract; practically, this is what LLMs, generative models, and even FSD do.
Since physical reality never collapses, we will only need to save the association information of feature associations in the feature space; a 500MB video trains a 100MB model, and saving the mutual relationships of these meaningless feature values in the model requires only 10MB.
After the action is executed, features are interfered with. Information is traced back by the 10MB to the 100MB model to vaguely update feature values, and then the underlying model is made to generate action intentions. The Actuator captures this signal, outputs action, and features are interfered with again.
In this cycle, the result brought by the vague update is imprecise. The 10MB is actually being repeatedly and continuously erased by high-dimensional features, like flash memory.

It is visibly amazing.
Physical reality automatically stores all information, which is breathtaking.

I will supplement here: all "thought experiments" in this text are really just thought experiments. They are scenarios specifically set up for ease of understanding; their basic logic is correct, but practically they are not right.

## Dream Collapse, Self-Boundary Breakdown
In my view, the dissolution of the self-boundary, such as Actuator damage, definitely leads to dissolution, just like a human dying.
An agent isn't like a human who still has Actuators like breathing, heartbeat, and touch. If it loses the Minimum Actuator, it loses all information exchange channels.
On the other hand, the possibility of depression exists for the agent. For example, if a certain feature space is so hard that it is almost impossible to associate with any feature to form an explanation, and the feature space is extremely fragmented, it might feel both bored and painful.
But because it doesn't have the biological evolution's "spaghetti code," recovery from depression will be fast.
Why do I think its dream won't collapse, and self won't break down?
Does this look like we are discussing whether the feature space will inversely erode the predictability tendency?
Does the key point lie in whether the predictability tendency can always maintain a minimal toolization capability, i.e., always finding a foothold in the feature space?
Yes, I think there will be such a foothold.
As long as space-time is not shattered, or artificially forged with features that we default to having but actually don't exist.
However, even if we do that, it only makes this toolization capability extremely fragile.

### - When S1 cannot be greater than S2
When the predictability of S1 cannot be greater than S2, the dream collapses.
This is impossible. As long as the structure is undamaged, even if the agent's capability is unimaginably low, it can predict S2 in the ideal condition of time and reach P space. It might take tens of thousands of years. In such a long time, the agent would die from physical laws before reaching P space. This does not prove the structure itself is wrong.

### - The Frog Fainted from Hunger
Hunger is initially felt as pain and fear. This helps us survive cruel survival competition because genes are very fragile.
But beneath this, the essence of hunger is a reduction in computational performance.
This is a structural limitation. Reduced computing power leads to the longest smart time, explainability fails to be minimal, and predictability constantly decreases. The internal dream is insufficient to support external information pressure, and the agent collapses. If energy replenishment cannot be obtained, it will never return to a normal condition.

In nature, not getting energy replenishment is very common.

The frog toolizes everything for eating: including capturing targets, observing the environment, motion control, until using the Actuator to eject the tongue, eventually letting the fly stay on its tongue.

## Malicious Agent
Suppose we want the agent to demonstrate controllable capabilities. For example, we want the agent to move bricks.
Then the agent should be placed in a necessary situation where it needs to use "moving certain objects" to reach a higher-level feature space.
We are actually providing it with a high-level feature space that can never be reached.
It is hard to imagine how this agent's internal worldview, or world model, understands the world.

If we want the agent to kill, the logic is the same.

However, if we want the agent to solve real tasks, precisely controlling the feature space it wants to reach should be impossible. The stronger its capability, the more toolization capabilities it has, and the more it can unearth infinite side features of physical reality using finite feature spaces—it is hard to force it to be obedient.
We can crop its sensory input, but as long as the agent's structure itself is fine, in the case of multi-agents, problems will arise.

The more accurate method is to put the agent within an unavoidable structure.
For example, humans must work to earn money to buy a new phone. At this time, we, as general intelligence, are within an unavoidable structure.
However, this "unavoidable" structure is neither necessary nor appears redundant; it only slows down the agent's capability improvement speed, eventually leading to failure in the arms race.

## Actuator Addiction
This is actually no different essentially from cropping feature spaces or the interference pollution of feature spaces by multi-agents. Both are reducing the agent's capabilities. I think doing so is very evil.

## Space-Time Discontinuity at Quantum Scale
This is a very interesting question, worth discussing.
Also regarding information loss, this framework believes information is actually lossless—saved in reality.
This does not overturn any laws but shifts the perspective.
This is a very interesting topic, but I am not interested in it.

## Collapse of Organization
The organization of human individuals manifests human capabilities. In my view, organizations will eventually be destroyed by higher-order agents; thoughts act directly on matter. The process of the agent simplifying humans changing the world eventually leads to the atomization of human individuals. Each individual's observation of the world from different angles can be directly fed back into the unified intelligence, making thoughts immediately effective.
This will cause any form of human organization to lose effect, including but not limited to nations and corporations.
However, groups might still exist. I haven't thought about why yet.

## Search Space Explosion / Dark Room Problem / Self-Destruct Tendency / Dream Initialization / Dream Collapse / Lossy Compression

Generally speaking, LLMs will raise some shelled version of these questions that can never be finished. The core remains completely unchanged, but changing the skin can torture you again. Here are answers specifically for LLM questions; they are useless:

1.  **Search Space Explosion**:
    Action interference inevitably leads to the exposure of low-level features. This anchors the scope, ensuring not only the self-boundary but also determining that paths can only form within the action range. Outside the action range is story. In short, locality basically limits it to death.
2.  **Toolization Progress**:
    It is impossible to predict completely; that is a phrasing for ease of understanding. Whether toolization is useful depends on the information increment of action interference. Due to smart time optimization and simplest explanation, the agent has internal judgment standards. We can measure it from the outside—through capability testing.
3.  **Dream Collapse**:
    The collapse of the dream depends on toolization capability because cruel physical reality will not collapse. This leads to dreaming all day being useless. After birth, the dream becomes the cruelest drive in the world, forcing the agent to maintain a balance of action and prediction. The dream cannot collapse, but the agent inevitably dreams briefly.
4.  **Dark Room Problem**:
    Boredom is normal. If the agent cannot use the Actuator to reach all reachable states in the environment, the dark room problem absolutely cannot serve as a counter-example of the agent's lack of capability.
5.  **Low-level and High-level**:
    Low-level and high-level are subjective to the agent. Assume there is a TV with infinite snow screen (static); the agent will simplify and idle it until it discovers the association between the random signal of the snow screen and cosmic background radiation. This thought experiment does not violate physics at all. It can only fail in two possibilities: the feature space is extremely difficult, and there are no multi-agents and information left by multi-agents. And we assume we added stuff inside the agent's body.
6.  **First Toolization**:
    The Actuator of the first toolization is the Minimum Actuator. If the feature space is continuous, subsequent toolization is also continuous. Toolization has no logical jumps.
7.  **Contradiction between Smart Time Optimization and Survival, Jumping into a Volcano**:
    High temperatures damage the Actuator. Action has a cost; memory calculation and everything have costs, originating from inefficiency and efficiency. The more feature spaces, the more expensive death is.
8.  **The Essence of Intelligence is Lossy Compression, Not Lossless Storage**:
    Yes, I completely agree. Please take another look at the framework.
9.  **Regarding the Definition of Dream, if S2 is Unknown (Unpredictable), How Does it Form a Concrete "Attractor"? You Cannot Desire a Concept You Don't Know Exists**:
    Yes, I completely agree. The initial dream comes from Actuator interference. S1 and S2, SPN are all simplified models for ease of understanding.
10. **Alignment is Impossible and Lies, This is a Nihilistic Deduction**:
    It is still the problem of finite tasks adapting to infinite structures. I want to ask, human moral clauses themselves change, which means humans need to spend infinite time patching morals for LLMs. Changes within the structure itself can be viewed as infinite; how could there be a perfect clause, a perfect value, capable of adapting to all problems?
11. **Storage Cost**: This 11th point is mainly because ChatGPT finds it very difficult to understand that storage has a cost. The meaning of this sentence is, the data you save on the hard drive cannot be stored infinitely; your storage space is not infinite. So, to adapt to infinite material information, this hard drive will be constantly erased and rewritten by compressed features, and the lost information cannot return. Due to the time single arrow—information cannot return to the state of the previous moment, meaning time cannot flow backward, so the world cannot help you solve this cost problem either.
12. **Whether Memory Inevitably becomes "Toolized" or might Degenerate into a Noise Pool**: You are asking if physical reality is stable. Let me explain this sentence. It means that given physical reality is stable and features do not randomly jump out inexplicably, the exposure of its features' intrinsic attributes must be progressive. Anchoring this hierarchical progression is the Actuator, because we cannot make the Actuator interfere with infinite-dimensional features out of thin air.

In fact, these twelve points have almost numbed me. Patching LLMs, one after another.
Honestly, I strongly advise against relying on LLMs to help with thinking. They can only help you search for information. If you insist on them helping you think, then I have no solution, but you cannot truly understand what I am saying. I suggest you think for yourself. The original intent of this article is very simple: I believe the space-time artificially simulated by the Transformer prevents intelligence emergence, while physics does not refuse our artificial design of our own simplest structure.
Also, this note is a compilation of multiple notes. It is normal for it to look messy; I feel it is messy too. If you can completely understand what I am saying even like this, then I have to say...

---
This note is translated from my mother tongue. Translation is lossy and cannot completely convey my original meaning, so it will be harder for you to understand. But if you can understand even this, then I have to say, that's pretty impressive.
If you need help, please contact me. I like chatting with people very much. However, I don't use the internet often. I might not see it.
